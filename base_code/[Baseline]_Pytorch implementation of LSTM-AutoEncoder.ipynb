{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "TRAIN_DF_RAW = pd.read_csv(\"./train.csv\")\n",
    "TEST_DF_RAW = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop([\"Timestamp\", 'anomaly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 과정\n",
    "TRN_MIN = TRAIN_DF_RAW[COLUMNS_IN_TRAIN_DATASET].min()\n",
    "TRN_MAX = TRAIN_DF_RAW[COLUMNS_IN_TRAIN_DATASET].max()\n",
    "\n",
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TRN_MAX[c] != TRN_MIN[c]:\n",
    "            ndf[c] = (df[c] - TRN_MIN[c]) / (TRN_MAX[c] - TRN_MIN[c])\n",
    "    return ndf\n",
    "\n",
    "TRAIN_DF = normalize(TRAIN_DF_RAW[COLUMNS_IN_TRAIN_DATASET])\n",
    "TEST_DF = normalize(TEST_DF_RAW[COLUMNS_IN_TRAIN_DATASET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 세팅\n",
    "WINDOW_GIVEN = 40\n",
    "WINDOW_SIZE = 41\n",
    "BATCH_SIZE = 1024 \n",
    "N_HIDDENS = 150\n",
    "N_HIDDENS_2 = 70\n",
    "N_LAYERS = 3\n",
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, ts, df, stride=1):\n",
    "        self.ts = np.array(ts)\n",
    "        self.vals = np.array(df, dtype=np.float32)\n",
    "        \n",
    "        self.valid_idx = np.arange(0, len(self.ts) - WINDOW_SIZE + 1, stride)\n",
    "        self.num_win = len(self.valid_idx)\n",
    "\n",
    "        self.pre_ts = self.ts[self.valid_idx + WINDOW_SIZE - 1]\n",
    "        self.pre_in = np.array([self.vals[i:i + WINDOW_GIVEN] for i in self.valid_idx])\n",
    "        self.pre_tgt = self.vals[self.valid_idx + WINDOW_SIZE - 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_win\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"timestamps\": self.pre_ts[idx],\n",
    "            \"input\": torch.from_numpy(self.pre_in[idx]),\n",
    "            \"target\": torch.from_numpy(self.pre_tgt[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN = TimeSeriesDataset(TRAIN_DF_RAW[\"Timestamp\"], TRAIN_DF, stride=1)\n",
    "TRAIN_LOADER = torch.utils.data.DataLoader(DATASET_TRAIN, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Linear(nn.Module):\n",
    "    def __init__(self, n_tags):\n",
    "        super(GRU_Linear, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=n_tags,\n",
    "            hidden_size=N_HIDDENS,\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.fc = nn.Linear(N_HIDDENS * 2, N_HIDDENS_2)\n",
    "        self.dense = nn.Linear(N_HIDDENS_2, n_tags)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_sequence = input_sequence.transpose(0, 1)\n",
    "        self.gru.flatten_parameters()\n",
    "        gru_outputs, _ = self.gru(input_sequence)\n",
    "        last_gru_output = gru_outputs[-1]\n",
    "        \n",
    "        output = self.fc(last_gru_output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dense(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = GRU_Linear(n_tags=TRAIN_DF.shape[1]).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, n_epochs, device):\n",
    "    train_losses = []\n",
    "    best_model = {\n",
    "        \"loss\": float('inf'),\n",
    "        \"state\": None,\n",
    "        \"epoch\": 0\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
    "            for batch in t:\n",
    "                inputs = batch[\"input\"].to(device)\n",
    "                targets = batch[\"target\"].to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        if avg_epoch_loss < best_model[\"loss\"]:\n",
    "            best_model[\"state\"] = model.state_dict()\n",
    "            best_model[\"loss\"] = avg_epoch_loss\n",
    "            best_model[\"epoch\"] = epoch + 1\n",
    "\n",
    "    return train_losses, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:17<00:00, 27.19batch/s, loss=0.0258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Train Loss: 0.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:17<00:00, 27.11batch/s, loss=0.0059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Train Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:17<00:00, 27.32batch/s, loss=0.00342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Train Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:18<00:00, 26.81batch/s, loss=0.00282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Train Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:18<00:00, 26.56batch/s, loss=0.00263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Train Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, best_model = train_model(MODEL, TRAIN_LOADER, optimizer, criterion, N_EPOCHS, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THRESHOLD\n",
    "- 훈련 데이터의 재구성 오차 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.eval()\n",
    "train_errors = []\n",
    "with torch.no_grad():\n",
    "    for batch in TRAIN_LOADER:\n",
    "        inputs = batch[\"input\"].cuda()\n",
    "        targets = batch[\"target\"].cuda()\n",
    "        outputs = MODEL(inputs)\n",
    "        errors = torch.mean(torch.abs(targets - outputs), dim=1).cpu().numpy()\n",
    "        train_errors.extend(errors)\n",
    "\n",
    "# 임계값 설정\n",
    "THRESHOLD = np.mean(train_errors) + 2 * np.std(train_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Detect anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TEST = TimeSeriesDataset(TEST_DF_RAW[\"Timestamp\"], TEST_DF)\n",
    "TEST_LOADER = torch.utils.data.DataLoader(DATASET_TEST, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 440/440 [00:08<00:00, 53.60batch/s]\n"
     ]
    }
   ],
   "source": [
    "def inference(model, data_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    timestamps = []\n",
    "    distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Inference\", unit=\"batch\"):\n",
    "            inputs = batch[\"input\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            timestamps.extend(batch[\"timestamps\"])\n",
    "            distances.extend(torch.abs(targets - predictions).cpu().tolist())\n",
    "\n",
    "    return np.array(timestamps), np.array(distances)\n",
    "\n",
    "timestamps, distances = inference(MODEL, TEST_LOADER)\n",
    "\n",
    "ANOMALY_SCORE = np.mean(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_labels(distance, threshold):\n",
    "    xs = np.zeros_like(distance)\n",
    "    xs[distance > threshold] = 1\n",
    "    return xs\n",
    "\n",
    "import datetime\n",
    "\n",
    "def fill_blank(check_ts, labels, total_ts):\n",
    "    TS_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    def parse_ts(ts):\n",
    "        return datetime.datetime.strptime(ts.strip(), TS_FORMAT)\n",
    "\n",
    "    def ts_label_iter():\n",
    "        return ((parse_ts(ts), label) for ts, label in zip(check_ts, labels))\n",
    "\n",
    "    final_labels = []\n",
    "    label_iter = ts_label_iter()\n",
    "    cur_ts, cur_label = next(label_iter, (None, None))\n",
    "\n",
    "    for ts in total_ts:\n",
    "        cur_time = parse_ts(ts)\n",
    "        while cur_ts and cur_time > cur_ts:\n",
    "            cur_ts, cur_label = next(label_iter, (None, None))\n",
    "        \n",
    "        if cur_ts == cur_time:\n",
    "            final_labels.append(cur_label)\n",
    "            cur_ts, cur_label = next(label_iter, (None, None))\n",
    "        else:\n",
    "            final_labels.append(0)\n",
    "\n",
    "    return np.array(final_labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD)\n",
    "PREDICTION = fill_blank(timestamps, LABELS, np.array(TEST_DF_RAW[\"Timestamp\"]))\n",
    "PREDICTION = PREDICTION.flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "sample_submission['anomaly'] = PREDICTION\n",
    "sample_submission.to_csv('./baseline_submission.csv', encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gimin_py38",
   "language": "python",
   "name": "gimin_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
